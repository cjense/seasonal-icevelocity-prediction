{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict velocity values using deep learning\n",
    "\n",
    "This notebook trains and uses an LSTM to predict velocity values. Modified from the code in [Encord's](https://encord.com/blog/time-series-predictions-with-recurrent-neural-networks/) post. An LSTM uses a memory cell to store information that the model uses to predict future values. Input, forget, and output gates control what is stored in, removed, and output from the LSTM. The LSTM's memory makes it ideal for predicting time series values; the lookback must be modified to encapsulate enough days to predict but not too many for overfitting.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load packages and open dataset\n",
    "\n",
    "Choose a point (`upstream`, `middle`, `terminus`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(364, 1)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "point_name = 'middle' # 'upstream', 'middle', or 'terminus'\n",
    "input_file='../data/ai_ready/' + point_name + '_timeseries.csv'\n",
    "\n",
    "# load the dataset, drop the series column\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "df.drop(columns=\"series\", inplace=True)\n",
    "dataset=df['velocity'].values.reshape(-1, 1)\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Preprocessing\n",
    "\n",
    "Here, we convert the values into a matrix, normalize, and split into train and test data. Normalization ensures that predictions are scaled properly so that one data point does not dominate training or predictions. We split into 80/20 train/test data and use a look back of 15, meaning that the model will consider the previous 15 values to predict the next value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert an array of values into a dataset matrix\n",
    "def create_dataset(dataset, look_back=1):\n",
    "\tdataX, dataY = [], []\n",
    "\tfor i in range(len(dataset)-look_back-1):\n",
    "\t\ta = dataset[i:(i+look_back), 0]\n",
    "\t\tdataX.append(a)\n",
    "\t\tdataY.append(dataset[i + look_back, 0])\n",
    "\tprint(len(dataX), len(dataY))\n",
    "\treturn np.array(dataX), np.array(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(364, 1)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalize the dataset\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "dataset = scaler.fit_transform(dataset)\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((254, 1), (110, 1))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split into train and test sets, 70% test data, 30% training data\n",
    "train_size = int(len(dataset) * 0.70)\n",
    "test_size = len(dataset) - train_size\n",
    "train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "248 248\n",
      "104 104\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((247, 5), (247,), (104, 5), (104,))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reshape into X=t and Y=t+1, timestep 5\n",
    "look_back = 5\n",
    "trainX, trainY = create_dataset(train, look_back)\n",
    "testX, testY = create_dataset(test, look_back)\n",
    "\n",
    "trainX = trainX[:-1]\n",
    "trainY = trainY[:-1]\n",
    "trainX.shape, trainY.shape, testX.shape, testY.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train and test\n",
    "\n",
    "Create the model and train for 5000 epochs. Save it to the `models` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((247, 5, 1), (247,), (104, 5, 1), (104,))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reshape input to be [samples, time steps, features]\n",
    "trainX = np.reshape(trainX, (trainX.shape[0], trainX.shape[1], 1))\n",
    "testX = np.reshape(testX, (testX.shape[0], testX.shape[1], 1))\n",
    "\n",
    "trainX.shape, trainY.shape, testX.shape, testY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cjense/miniconda3/envs/mlgeo-jensencc/lib/python3.10/site-packages/keras/layers/rnn/rnn.py:205: UserWarning:\n",
      "\n",
      "Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 - 2s - 258ms/step - loss: 0.2051 - val_loss: 0.0869\n",
      "Epoch 2/500\n",
      "8/8 - 0s - 8ms/step - loss: 0.0466 - val_loss: 0.0238\n",
      "Epoch 3/500\n",
      "8/8 - 0s - 10ms/step - loss: 0.0287 - val_loss: 0.0429\n",
      "Epoch 4/500\n",
      "8/8 - 0s - 10ms/step - loss: 0.0303 - val_loss: 0.0149\n",
      "Epoch 5/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0209 - val_loss: 0.0116\n",
      "Epoch 6/500\n",
      "8/8 - 0s - 8ms/step - loss: 0.0214 - val_loss: 0.0193\n",
      "Epoch 7/500\n",
      "8/8 - 0s - 8ms/step - loss: 0.0202 - val_loss: 0.0129\n",
      "Epoch 8/500\n",
      "8/8 - 0s - 8ms/step - loss: 0.0201 - val_loss: 0.0130\n",
      "Epoch 9/500\n",
      "8/8 - 0s - 8ms/step - loss: 0.0181 - val_loss: 0.0188\n",
      "Epoch 10/500\n",
      "8/8 - 0s - 8ms/step - loss: 0.0175 - val_loss: 0.0119\n",
      "Epoch 11/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0203 - val_loss: 0.0141\n",
      "Epoch 12/500\n",
      "8/8 - 0s - 10ms/step - loss: 0.0198 - val_loss: 0.0150\n",
      "Epoch 13/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0187 - val_loss: 0.0122\n",
      "Epoch 14/500\n",
      "8/8 - 0s - 10ms/step - loss: 0.0186 - val_loss: 0.0151\n",
      "Epoch 15/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0204 - val_loss: 0.0133\n",
      "Epoch 16/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0198 - val_loss: 0.0141\n",
      "Epoch 17/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0200 - val_loss: 0.0154\n",
      "Epoch 18/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0178 - val_loss: 0.0126\n",
      "Epoch 19/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0207 - val_loss: 0.0140\n",
      "Epoch 20/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0201 - val_loss: 0.0137\n",
      "Epoch 21/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0194 - val_loss: 0.0142\n",
      "Epoch 22/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0188 - val_loss: 0.0130\n",
      "Epoch 23/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0195 - val_loss: 0.0141\n",
      "Epoch 24/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0181 - val_loss: 0.0133\n",
      "Epoch 25/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0199 - val_loss: 0.0144\n",
      "Epoch 26/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0193 - val_loss: 0.0139\n",
      "Epoch 27/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0187 - val_loss: 0.0133\n",
      "Epoch 28/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0194 - val_loss: 0.0154\n",
      "Epoch 29/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0171 - val_loss: 0.0124\n",
      "Epoch 30/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0186 - val_loss: 0.0146\n",
      "Epoch 31/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0191 - val_loss: 0.0141\n",
      "Epoch 32/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0194 - val_loss: 0.0144\n",
      "Epoch 33/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0199 - val_loss: 0.0131\n",
      "Epoch 34/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0181 - val_loss: 0.0135\n",
      "Epoch 35/500\n",
      "8/8 - 0s - 10ms/step - loss: 0.0193 - val_loss: 0.0125\n",
      "Epoch 36/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0197 - val_loss: 0.0148\n",
      "Epoch 37/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0192 - val_loss: 0.0137\n",
      "Epoch 38/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0188 - val_loss: 0.0156\n",
      "Epoch 39/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0195 - val_loss: 0.0129\n",
      "Epoch 40/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0201 - val_loss: 0.0141\n",
      "Epoch 41/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0201 - val_loss: 0.0146\n",
      "Epoch 42/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0195 - val_loss: 0.0150\n",
      "Epoch 43/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0187 - val_loss: 0.0131\n",
      "Epoch 44/500\n",
      "8/8 - 0s - 17ms/step - loss: 0.0190 - val_loss: 0.0133\n",
      "Epoch 45/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0189 - val_loss: 0.0132\n",
      "Epoch 46/500\n",
      "8/8 - 0s - 12ms/step - loss: 0.0182 - val_loss: 0.0139\n",
      "Epoch 47/500\n",
      "8/8 - 0s - 10ms/step - loss: 0.0197 - val_loss: 0.0144\n",
      "Epoch 48/500\n",
      "8/8 - 0s - 10ms/step - loss: 0.0184 - val_loss: 0.0127\n",
      "Epoch 49/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0189 - val_loss: 0.0154\n",
      "Epoch 50/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0200 - val_loss: 0.0129\n",
      "Epoch 51/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0189 - val_loss: 0.0130\n",
      "Epoch 52/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0196 - val_loss: 0.0158\n",
      "Epoch 53/500\n",
      "8/8 - 0s - 8ms/step - loss: 0.0172 - val_loss: 0.0118\n",
      "Epoch 54/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0176 - val_loss: 0.0148\n",
      "Epoch 55/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0194 - val_loss: 0.0128\n",
      "Epoch 56/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0191 - val_loss: 0.0138\n",
      "Epoch 57/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0192 - val_loss: 0.0133\n",
      "Epoch 58/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0197 - val_loss: 0.0133\n",
      "Epoch 59/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0187 - val_loss: 0.0122\n",
      "Epoch 60/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0198 - val_loss: 0.0126\n",
      "Epoch 61/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0199 - val_loss: 0.0155\n",
      "Epoch 62/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0188 - val_loss: 0.0116\n",
      "Epoch 63/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0190 - val_loss: 0.0143\n",
      "Epoch 64/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0182 - val_loss: 0.0127\n",
      "Epoch 65/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0190 - val_loss: 0.0147\n",
      "Epoch 66/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0194 - val_loss: 0.0127\n",
      "Epoch 67/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0195 - val_loss: 0.0153\n",
      "Epoch 68/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0174 - val_loss: 0.0122\n",
      "Epoch 69/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0193 - val_loss: 0.0155\n",
      "Epoch 70/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0189 - val_loss: 0.0117\n",
      "Epoch 71/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0181 - val_loss: 0.0153\n",
      "Epoch 72/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0195 - val_loss: 0.0118\n",
      "Epoch 73/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0187 - val_loss: 0.0143\n",
      "Epoch 74/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0198 - val_loss: 0.0173\n",
      "Epoch 75/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0187 - val_loss: 0.0132\n",
      "Epoch 76/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0184 - val_loss: 0.0124\n",
      "Epoch 77/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0178 - val_loss: 0.0130\n",
      "Epoch 78/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0179 - val_loss: 0.0141\n",
      "Epoch 79/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0189 - val_loss: 0.0131\n",
      "Epoch 80/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0186 - val_loss: 0.0126\n",
      "Epoch 81/500\n",
      "8/8 - 0s - 8ms/step - loss: 0.0174 - val_loss: 0.0144\n",
      "Epoch 82/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0190 - val_loss: 0.0114\n",
      "Epoch 83/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0181 - val_loss: 0.0167\n",
      "Epoch 84/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0195 - val_loss: 0.0126\n",
      "Epoch 85/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0175 - val_loss: 0.0128\n",
      "Epoch 86/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0177 - val_loss: 0.0122\n",
      "Epoch 87/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0169 - val_loss: 0.0142\n",
      "Epoch 88/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0166 - val_loss: 0.0122\n",
      "Epoch 89/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0193 - val_loss: 0.0164\n",
      "Epoch 90/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0176 - val_loss: 0.0111\n",
      "Epoch 91/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0177 - val_loss: 0.0168\n",
      "Epoch 92/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0189 - val_loss: 0.0109\n",
      "Epoch 93/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0182 - val_loss: 0.0144\n",
      "Epoch 94/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0199 - val_loss: 0.0150\n",
      "Epoch 95/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0191 - val_loss: 0.0113\n",
      "Epoch 96/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0190 - val_loss: 0.0125\n",
      "Epoch 97/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0190 - val_loss: 0.0159\n",
      "Epoch 98/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0207 - val_loss: 0.0115\n",
      "Epoch 99/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0198 - val_loss: 0.0158\n",
      "Epoch 100/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0177 - val_loss: 0.0122\n",
      "Epoch 101/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0188 - val_loss: 0.0156\n",
      "Epoch 102/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0176 - val_loss: 0.0126\n",
      "Epoch 103/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0173 - val_loss: 0.0145\n",
      "Epoch 104/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0174 - val_loss: 0.0112\n",
      "Epoch 105/500\n",
      "8/8 - 0s - 10ms/step - loss: 0.0180 - val_loss: 0.0144\n",
      "Epoch 106/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0184 - val_loss: 0.0129\n",
      "Epoch 107/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0173 - val_loss: 0.0115\n",
      "Epoch 108/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0172 - val_loss: 0.0132\n",
      "Epoch 109/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0188 - val_loss: 0.0139\n",
      "Epoch 110/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0172 - val_loss: 0.0110\n",
      "Epoch 111/500\n",
      "8/8 - 0s - 10ms/step - loss: 0.0183 - val_loss: 0.0118\n",
      "Epoch 112/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0174 - val_loss: 0.0126\n",
      "Epoch 113/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0193 - val_loss: 0.0122\n",
      "Epoch 114/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0170 - val_loss: 0.0128\n",
      "Epoch 115/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0170 - val_loss: 0.0122\n",
      "Epoch 116/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0174 - val_loss: 0.0148\n",
      "Epoch 117/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0177 - val_loss: 0.0105\n",
      "Epoch 118/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0165 - val_loss: 0.0138\n",
      "Epoch 119/500\n",
      "8/8 - 0s - 19ms/step - loss: 0.0176 - val_loss: 0.0138\n",
      "Epoch 120/500\n",
      "8/8 - 0s - 14ms/step - loss: 0.0175 - val_loss: 0.0102\n",
      "Epoch 121/500\n",
      "8/8 - 0s - 10ms/step - loss: 0.0183 - val_loss: 0.0134\n",
      "Epoch 122/500\n",
      "8/8 - 0s - 10ms/step - loss: 0.0172 - val_loss: 0.0121\n",
      "Epoch 123/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0178 - val_loss: 0.0134\n",
      "Epoch 124/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0166 - val_loss: 0.0110\n",
      "Epoch 125/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0176 - val_loss: 0.0121\n",
      "Epoch 126/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0188 - val_loss: 0.0123\n",
      "Epoch 127/500\n",
      "8/8 - 0s - 17ms/step - loss: 0.0166 - val_loss: 0.0106\n",
      "Epoch 128/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0180 - val_loss: 0.0130\n",
      "Epoch 129/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0193 - val_loss: 0.0100\n",
      "Epoch 130/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0177 - val_loss: 0.0141\n",
      "Epoch 131/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0157 - val_loss: 0.0101\n",
      "Epoch 132/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0181 - val_loss: 0.0134\n",
      "Epoch 133/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0156 - val_loss: 0.0101\n",
      "Epoch 134/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0179 - val_loss: 0.0121\n",
      "Epoch 135/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0163 - val_loss: 0.0122\n",
      "Epoch 136/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0171 - val_loss: 0.0115\n",
      "Epoch 137/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0164 - val_loss: 0.0116\n",
      "Epoch 138/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0166 - val_loss: 0.0117\n",
      "Epoch 139/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0161 - val_loss: 0.0121\n",
      "Epoch 140/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0160 - val_loss: 0.0110\n",
      "Epoch 141/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0168 - val_loss: 0.0108\n",
      "Epoch 142/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0170 - val_loss: 0.0108\n",
      "Epoch 143/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0165 - val_loss: 0.0119\n",
      "Epoch 144/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0190 - val_loss: 0.0096\n",
      "Epoch 145/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0173 - val_loss: 0.0128\n",
      "Epoch 146/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0168 - val_loss: 0.0106\n",
      "Epoch 147/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0158 - val_loss: 0.0108\n",
      "Epoch 148/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0178 - val_loss: 0.0130\n",
      "Epoch 149/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0182 - val_loss: 0.0096\n",
      "Epoch 150/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0168 - val_loss: 0.0149\n",
      "Epoch 151/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0159 - val_loss: 0.0096\n",
      "Epoch 152/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0161 - val_loss: 0.0140\n",
      "Epoch 153/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0175 - val_loss: 0.0098\n",
      "Epoch 154/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0169 - val_loss: 0.0138\n",
      "Epoch 155/500\n",
      "8/8 - 0s - 8ms/step - loss: 0.0170 - val_loss: 0.0098\n",
      "Epoch 156/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0186 - val_loss: 0.0131\n",
      "Epoch 157/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0166 - val_loss: 0.0116\n",
      "Epoch 158/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0159 - val_loss: 0.0101\n",
      "Epoch 159/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0172 - val_loss: 0.0118\n",
      "Epoch 160/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0160 - val_loss: 0.0095\n",
      "Epoch 161/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0155 - val_loss: 0.0115\n",
      "Epoch 162/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0155 - val_loss: 0.0098\n",
      "Epoch 163/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0174 - val_loss: 0.0119\n",
      "Epoch 164/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0169 - val_loss: 0.0105\n",
      "Epoch 165/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0155 - val_loss: 0.0103\n",
      "Epoch 166/500\n",
      "8/8 - 0s - 13ms/step - loss: 0.0173 - val_loss: 0.0093\n",
      "Epoch 167/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0169 - val_loss: 0.0150\n",
      "Epoch 168/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0163 - val_loss: 0.0090\n",
      "Epoch 169/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0167 - val_loss: 0.0150\n",
      "Epoch 170/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0168 - val_loss: 0.0090\n",
      "Epoch 171/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0169 - val_loss: 0.0117\n",
      "Epoch 172/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0170 - val_loss: 0.0094\n",
      "Epoch 173/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0195 - val_loss: 0.0129\n",
      "Epoch 174/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0174 - val_loss: 0.0095\n",
      "Epoch 175/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0151 - val_loss: 0.0097\n",
      "Epoch 176/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0159 - val_loss: 0.0103\n",
      "Epoch 177/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0170 - val_loss: 0.0101\n",
      "Epoch 178/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0165 - val_loss: 0.0117\n",
      "Epoch 179/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0166 - val_loss: 0.0094\n",
      "Epoch 180/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0156 - val_loss: 0.0118\n",
      "Epoch 181/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0163 - val_loss: 0.0099\n",
      "Epoch 182/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0149 - val_loss: 0.0096\n",
      "Epoch 183/500\n",
      "8/8 - 0s - 8ms/step - loss: 0.0146 - val_loss: 0.0105\n",
      "Epoch 184/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0161 - val_loss: 0.0103\n",
      "Epoch 185/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0173 - val_loss: 0.0113\n",
      "Epoch 186/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0154 - val_loss: 0.0097\n",
      "Epoch 187/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0146 - val_loss: 0.0089\n",
      "Epoch 188/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0146 - val_loss: 0.0099\n",
      "Epoch 189/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0147 - val_loss: 0.0093\n",
      "Epoch 190/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0158 - val_loss: 0.0116\n",
      "Epoch 191/500\n",
      "8/8 - 0s - 8ms/step - loss: 0.0156 - val_loss: 0.0087\n",
      "Epoch 192/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0167 - val_loss: 0.0118\n",
      "Epoch 193/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0153 - val_loss: 0.0092\n",
      "Epoch 194/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0166 - val_loss: 0.0095\n",
      "Epoch 195/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0164 - val_loss: 0.0116\n",
      "Epoch 196/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0173 - val_loss: 0.0094\n",
      "Epoch 197/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0159 - val_loss: 0.0100\n",
      "Epoch 198/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0168 - val_loss: 0.0108\n",
      "Epoch 199/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0147 - val_loss: 0.0088\n",
      "Epoch 200/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0156 - val_loss: 0.0108\n",
      "Epoch 201/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0152 - val_loss: 0.0093\n",
      "Epoch 202/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0144 - val_loss: 0.0093\n",
      "Epoch 203/500\n",
      "8/8 - 0s - 8ms/step - loss: 0.0158 - val_loss: 0.0099\n",
      "Epoch 204/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0158 - val_loss: 0.0094\n",
      "Epoch 205/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0157 - val_loss: 0.0114\n",
      "Epoch 206/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0167 - val_loss: 0.0103\n",
      "Epoch 207/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0161 - val_loss: 0.0089\n",
      "Epoch 208/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0150 - val_loss: 0.0091\n",
      "Epoch 209/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0157 - val_loss: 0.0114\n",
      "Epoch 210/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0159 - val_loss: 0.0095\n",
      "Epoch 211/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0153 - val_loss: 0.0091\n",
      "Epoch 212/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0164 - val_loss: 0.0098\n",
      "Epoch 213/500\n",
      "8/8 - 0s - 10ms/step - loss: 0.0155 - val_loss: 0.0088\n",
      "Epoch 214/500\n",
      "8/8 - 0s - 10ms/step - loss: 0.0154 - val_loss: 0.0110\n",
      "Epoch 215/500\n",
      "8/8 - 0s - 21ms/step - loss: 0.0151 - val_loss: 0.0097\n",
      "Epoch 216/500\n",
      "8/8 - 0s - 11ms/step - loss: 0.0149 - val_loss: 0.0106\n",
      "Epoch 217/500\n",
      "8/8 - 0s - 10ms/step - loss: 0.0149 - val_loss: 0.0096\n",
      "Epoch 218/500\n",
      "8/8 - 0s - 10ms/step - loss: 0.0161 - val_loss: 0.0095\n",
      "Epoch 219/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0151 - val_loss: 0.0106\n",
      "Epoch 220/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0154 - val_loss: 0.0095\n",
      "Epoch 221/500\n",
      "8/8 - 0s - 11ms/step - loss: 0.0141 - val_loss: 0.0096\n",
      "Epoch 222/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0155 - val_loss: 0.0090\n",
      "Epoch 223/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0153 - val_loss: 0.0093\n",
      "Epoch 224/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0156 - val_loss: 0.0102\n",
      "Epoch 225/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0144 - val_loss: 0.0108\n",
      "Epoch 226/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0155 - val_loss: 0.0095\n",
      "Epoch 227/500\n",
      "8/8 - 0s - 12ms/step - loss: 0.0148 - val_loss: 0.0088\n",
      "Epoch 228/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0148 - val_loss: 0.0100\n",
      "Epoch 229/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0141 - val_loss: 0.0086\n",
      "Epoch 230/500\n",
      "8/8 - 0s - 12ms/step - loss: 0.0143 - val_loss: 0.0117\n",
      "Epoch 231/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0156 - val_loss: 0.0098\n",
      "Epoch 232/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0147 - val_loss: 0.0088\n",
      "Epoch 233/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0151 - val_loss: 0.0094\n",
      "Epoch 234/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0146 - val_loss: 0.0088\n",
      "Epoch 235/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0151 - val_loss: 0.0097\n",
      "Epoch 236/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0147 - val_loss: 0.0099\n",
      "Epoch 237/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0149 - val_loss: 0.0093\n",
      "Epoch 238/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0142 - val_loss: 0.0086\n",
      "Epoch 239/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0155 - val_loss: 0.0094\n",
      "Epoch 240/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0136 - val_loss: 0.0094\n",
      "Epoch 241/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0143 - val_loss: 0.0104\n",
      "Epoch 242/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0142 - val_loss: 0.0086\n",
      "Epoch 243/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0150 - val_loss: 0.0098\n",
      "Epoch 244/500\n",
      "8/8 - 0s - 11ms/step - loss: 0.0147 - val_loss: 0.0094\n",
      "Epoch 245/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0145 - val_loss: 0.0089\n",
      "Epoch 246/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0147 - val_loss: 0.0095\n",
      "Epoch 247/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0141 - val_loss: 0.0093\n",
      "Epoch 248/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0143 - val_loss: 0.0092\n",
      "Epoch 249/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0148 - val_loss: 0.0092\n",
      "Epoch 250/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0152 - val_loss: 0.0099\n",
      "Epoch 251/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0166 - val_loss: 0.0097\n",
      "Epoch 252/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0153 - val_loss: 0.0086\n",
      "Epoch 253/500\n",
      "8/8 - 0s - 17ms/step - loss: 0.0155 - val_loss: 0.0111\n",
      "Epoch 254/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0152 - val_loss: 0.0087\n",
      "Epoch 255/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0147 - val_loss: 0.0098\n",
      "Epoch 256/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0150 - val_loss: 0.0106\n",
      "Epoch 257/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0143 - val_loss: 0.0084\n",
      "Epoch 258/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0153 - val_loss: 0.0095\n",
      "Epoch 259/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0155 - val_loss: 0.0083\n",
      "Epoch 260/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0152 - val_loss: 0.0087\n",
      "Epoch 261/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0144 - val_loss: 0.0093\n",
      "Epoch 262/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0144 - val_loss: 0.0090\n",
      "Epoch 263/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0145 - val_loss: 0.0104\n",
      "Epoch 264/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0143 - val_loss: 0.0086\n",
      "Epoch 265/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0153 - val_loss: 0.0102\n",
      "Epoch 266/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0161 - val_loss: 0.0097\n",
      "Epoch 267/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0156 - val_loss: 0.0093\n",
      "Epoch 268/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0138 - val_loss: 0.0090\n",
      "Epoch 269/500\n",
      "8/8 - 0s - 8ms/step - loss: 0.0140 - val_loss: 0.0098\n",
      "Epoch 270/500\n",
      "8/8 - 0s - 8ms/step - loss: 0.0152 - val_loss: 0.0088\n",
      "Epoch 271/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0142 - val_loss: 0.0106\n",
      "Epoch 272/500\n",
      "8/8 - 0s - 10ms/step - loss: 0.0140 - val_loss: 0.0084\n",
      "Epoch 273/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0145 - val_loss: 0.0102\n",
      "Epoch 274/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0149 - val_loss: 0.0098\n",
      "Epoch 275/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0140 - val_loss: 0.0094\n",
      "Epoch 276/500\n",
      "8/8 - 0s - 8ms/step - loss: 0.0139 - val_loss: 0.0096\n",
      "Epoch 277/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0134 - val_loss: 0.0085\n",
      "Epoch 278/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0144 - val_loss: 0.0100\n",
      "Epoch 279/500\n",
      "8/8 - 0s - 8ms/step - loss: 0.0138 - val_loss: 0.0091\n",
      "Epoch 280/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0146 - val_loss: 0.0089\n",
      "Epoch 281/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0166 - val_loss: 0.0098\n",
      "Epoch 282/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0152 - val_loss: 0.0090\n",
      "Epoch 283/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0150 - val_loss: 0.0111\n",
      "Epoch 284/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0145 - val_loss: 0.0091\n",
      "Epoch 285/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0141 - val_loss: 0.0089\n",
      "Epoch 286/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0157 - val_loss: 0.0088\n",
      "Epoch 287/500\n",
      "8/8 - 0s - 8ms/step - loss: 0.0140 - val_loss: 0.0098\n",
      "Epoch 288/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0149 - val_loss: 0.0092\n",
      "Epoch 289/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0149 - val_loss: 0.0098\n",
      "Epoch 290/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0141 - val_loss: 0.0088\n",
      "Epoch 291/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0146 - val_loss: 0.0101\n",
      "Epoch 292/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0150 - val_loss: 0.0088\n",
      "Epoch 293/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0134 - val_loss: 0.0095\n",
      "Epoch 294/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0155 - val_loss: 0.0096\n",
      "Epoch 295/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0150 - val_loss: 0.0112\n",
      "Epoch 296/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0145 - val_loss: 0.0095\n",
      "Epoch 297/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0150 - val_loss: 0.0089\n",
      "Epoch 298/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0137 - val_loss: 0.0096\n",
      "Epoch 299/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0152 - val_loss: 0.0110\n",
      "Epoch 300/500\n",
      "8/8 - 0s - 8ms/step - loss: 0.0146 - val_loss: 0.0095\n",
      "Epoch 301/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0142 - val_loss: 0.0096\n",
      "Epoch 302/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0142 - val_loss: 0.0094\n",
      "Epoch 303/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0139 - val_loss: 0.0098\n",
      "Epoch 304/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0151 - val_loss: 0.0093\n",
      "Epoch 305/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0137 - val_loss: 0.0094\n",
      "Epoch 306/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0139 - val_loss: 0.0101\n",
      "Epoch 307/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0150 - val_loss: 0.0089\n",
      "Epoch 308/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0152 - val_loss: 0.0091\n",
      "Epoch 309/500\n",
      "8/8 - 0s - 8ms/step - loss: 0.0140 - val_loss: 0.0095\n",
      "Epoch 310/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0154 - val_loss: 0.0086\n",
      "Epoch 311/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0158 - val_loss: 0.0112\n",
      "Epoch 312/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0147 - val_loss: 0.0086\n",
      "Epoch 313/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0142 - val_loss: 0.0093\n",
      "Epoch 314/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0144 - val_loss: 0.0094\n",
      "Epoch 315/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0138 - val_loss: 0.0095\n",
      "Epoch 316/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0156 - val_loss: 0.0092\n",
      "Epoch 317/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0135 - val_loss: 0.0091\n",
      "Epoch 318/500\n",
      "8/8 - 0s - 11ms/step - loss: 0.0131 - val_loss: 0.0087\n",
      "Epoch 319/500\n",
      "8/8 - 0s - 10ms/step - loss: 0.0144 - val_loss: 0.0108\n",
      "Epoch 320/500\n",
      "8/8 - 0s - 10ms/step - loss: 0.0140 - val_loss: 0.0098\n",
      "Epoch 321/500\n",
      "8/8 - 0s - 14ms/step - loss: 0.0145 - val_loss: 0.0093\n",
      "Epoch 322/500\n",
      "8/8 - 0s - 11ms/step - loss: 0.0133 - val_loss: 0.0089\n",
      "Epoch 323/500\n",
      "8/8 - 0s - 10ms/step - loss: 0.0144 - val_loss: 0.0096\n",
      "Epoch 324/500\n",
      "8/8 - 0s - 11ms/step - loss: 0.0147 - val_loss: 0.0093\n",
      "Epoch 325/500\n",
      "8/8 - 0s - 10ms/step - loss: 0.0153 - val_loss: 0.0091\n",
      "Epoch 326/500\n",
      "8/8 - 0s - 13ms/step - loss: 0.0148 - val_loss: 0.0105\n",
      "Epoch 327/500\n",
      "8/8 - 0s - 10ms/step - loss: 0.0144 - val_loss: 0.0091\n",
      "Epoch 328/500\n",
      "8/8 - 0s - 10ms/step - loss: 0.0138 - val_loss: 0.0087\n",
      "Epoch 329/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0138 - val_loss: 0.0096\n",
      "Epoch 330/500\n",
      "8/8 - 0s - 11ms/step - loss: 0.0142 - val_loss: 0.0097\n",
      "Epoch 331/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0133 - val_loss: 0.0098\n",
      "Epoch 332/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0137 - val_loss: 0.0089\n",
      "Epoch 333/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0140 - val_loss: 0.0093\n",
      "Epoch 334/500\n",
      "8/8 - 0s - 10ms/step - loss: 0.0145 - val_loss: 0.0098\n",
      "Epoch 335/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0132 - val_loss: 0.0094\n",
      "Epoch 336/500\n",
      "8/8 - 0s - 17ms/step - loss: 0.0151 - val_loss: 0.0092\n",
      "Epoch 337/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0139 - val_loss: 0.0094\n",
      "Epoch 338/500\n",
      "8/8 - 0s - 13ms/step - loss: 0.0132 - val_loss: 0.0092\n",
      "Epoch 339/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0127 - val_loss: 0.0090\n",
      "Epoch 340/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0148 - val_loss: 0.0090\n",
      "Epoch 341/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0147 - val_loss: 0.0093\n",
      "Epoch 342/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0139 - val_loss: 0.0096\n",
      "Epoch 343/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0145 - val_loss: 0.0098\n",
      "Epoch 344/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0148 - val_loss: 0.0095\n",
      "Epoch 345/500\n",
      "8/8 - 0s - 8ms/step - loss: 0.0146 - val_loss: 0.0095\n",
      "Epoch 346/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0137 - val_loss: 0.0093\n",
      "Epoch 347/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0138 - val_loss: 0.0087\n",
      "Epoch 348/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0140 - val_loss: 0.0092\n",
      "Epoch 349/500\n",
      "8/8 - 0s - 8ms/step - loss: 0.0135 - val_loss: 0.0091\n",
      "Epoch 350/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0144 - val_loss: 0.0093\n",
      "Epoch 351/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0135 - val_loss: 0.0090\n",
      "Epoch 352/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0132 - val_loss: 0.0096\n",
      "Epoch 353/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0144 - val_loss: 0.0096\n",
      "Epoch 354/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0148 - val_loss: 0.0090\n",
      "Epoch 355/500\n",
      "8/8 - 0s - 8ms/step - loss: 0.0136 - val_loss: 0.0094\n",
      "Epoch 356/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0143 - val_loss: 0.0097\n",
      "Epoch 357/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0132 - val_loss: 0.0093\n",
      "Epoch 358/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0135 - val_loss: 0.0090\n",
      "Epoch 359/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0136 - val_loss: 0.0100\n",
      "Epoch 360/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0142 - val_loss: 0.0092\n",
      "Epoch 361/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0147 - val_loss: 0.0093\n",
      "Epoch 362/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0145 - val_loss: 0.0089\n",
      "Epoch 363/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0138 - val_loss: 0.0103\n",
      "Epoch 364/500\n",
      "8/8 - 0s - 12ms/step - loss: 0.0138 - val_loss: 0.0090\n",
      "Epoch 365/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0144 - val_loss: 0.0090\n",
      "Epoch 366/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0135 - val_loss: 0.0098\n",
      "Epoch 367/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0147 - val_loss: 0.0091\n",
      "Epoch 368/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0140 - val_loss: 0.0094\n",
      "Epoch 369/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0137 - val_loss: 0.0093\n",
      "Epoch 370/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0134 - val_loss: 0.0093\n",
      "Epoch 371/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0139 - val_loss: 0.0094\n",
      "Epoch 372/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0144 - val_loss: 0.0094\n",
      "Epoch 373/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0143 - val_loss: 0.0093\n",
      "Epoch 374/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0136 - val_loss: 0.0094\n",
      "Epoch 375/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0135 - val_loss: 0.0091\n",
      "Epoch 376/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0130 - val_loss: 0.0089\n",
      "Epoch 377/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0135 - val_loss: 0.0101\n",
      "Epoch 378/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0145 - val_loss: 0.0094\n",
      "Epoch 379/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0136 - val_loss: 0.0091\n",
      "Epoch 380/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0138 - val_loss: 0.0097\n",
      "Epoch 381/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0132 - val_loss: 0.0093\n",
      "Epoch 382/500\n",
      "8/8 - 0s - 10ms/step - loss: 0.0138 - val_loss: 0.0095\n",
      "Epoch 383/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0141 - val_loss: 0.0095\n",
      "Epoch 384/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0136 - val_loss: 0.0098\n",
      "Epoch 385/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0134 - val_loss: 0.0095\n",
      "Epoch 386/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0145 - val_loss: 0.0091\n",
      "Epoch 387/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0128 - val_loss: 0.0094\n",
      "Epoch 388/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0130 - val_loss: 0.0093\n",
      "Epoch 389/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0131 - val_loss: 0.0104\n",
      "Epoch 390/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0134 - val_loss: 0.0094\n",
      "Epoch 391/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0141 - val_loss: 0.0097\n",
      "Epoch 392/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0133 - val_loss: 0.0093\n",
      "Epoch 393/500\n",
      "8/8 - 0s - 11ms/step - loss: 0.0142 - val_loss: 0.0100\n",
      "Epoch 394/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0136 - val_loss: 0.0097\n",
      "Epoch 395/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0142 - val_loss: 0.0097\n",
      "Epoch 396/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0131 - val_loss: 0.0096\n",
      "Epoch 397/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0136 - val_loss: 0.0098\n",
      "Epoch 398/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0127 - val_loss: 0.0100\n",
      "Epoch 399/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0133 - val_loss: 0.0103\n",
      "Epoch 400/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0137 - val_loss: 0.0094\n",
      "Epoch 401/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0133 - val_loss: 0.0100\n",
      "Epoch 402/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0129 - val_loss: 0.0098\n",
      "Epoch 403/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0132 - val_loss: 0.0093\n",
      "Epoch 404/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0123 - val_loss: 0.0098\n",
      "Epoch 405/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0131 - val_loss: 0.0097\n",
      "Epoch 406/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0128 - val_loss: 0.0093\n",
      "Epoch 407/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0135 - val_loss: 0.0096\n",
      "Epoch 408/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0145 - val_loss: 0.0095\n",
      "Epoch 409/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0136 - val_loss: 0.0096\n",
      "Epoch 410/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0148 - val_loss: 0.0101\n",
      "Epoch 411/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0133 - val_loss: 0.0098\n",
      "Epoch 412/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0133 - val_loss: 0.0094\n",
      "Epoch 413/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0136 - val_loss: 0.0097\n",
      "Epoch 414/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0131 - val_loss: 0.0104\n",
      "Epoch 415/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0138 - val_loss: 0.0097\n",
      "Epoch 416/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0134 - val_loss: 0.0099\n",
      "Epoch 417/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0134 - val_loss: 0.0095\n",
      "Epoch 418/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0125 - val_loss: 0.0097\n",
      "Epoch 419/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0130 - val_loss: 0.0095\n",
      "Epoch 420/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0127 - val_loss: 0.0098\n",
      "Epoch 421/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0130 - val_loss: 0.0098\n",
      "Epoch 422/500\n",
      "8/8 - 0s - 10ms/step - loss: 0.0133 - val_loss: 0.0094\n",
      "Epoch 423/500\n",
      "8/8 - 0s - 12ms/step - loss: 0.0137 - val_loss: 0.0100\n",
      "Epoch 424/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0121 - val_loss: 0.0099\n",
      "Epoch 425/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0129 - val_loss: 0.0098\n",
      "Epoch 426/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0132 - val_loss: 0.0099\n",
      "Epoch 427/500\n",
      "8/8 - 0s - 17ms/step - loss: 0.0135 - val_loss: 0.0107\n",
      "Epoch 428/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0141 - val_loss: 0.0098\n",
      "Epoch 429/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0132 - val_loss: 0.0096\n",
      "Epoch 430/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0135 - val_loss: 0.0100\n",
      "Epoch 431/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0135 - val_loss: 0.0095\n",
      "Epoch 432/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0118 - val_loss: 0.0092\n",
      "Epoch 433/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0125 - val_loss: 0.0103\n",
      "Epoch 434/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0134 - val_loss: 0.0095\n",
      "Epoch 435/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0133 - val_loss: 0.0097\n",
      "Epoch 436/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0142 - val_loss: 0.0098\n",
      "Epoch 437/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0133 - val_loss: 0.0109\n",
      "Epoch 438/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0128 - val_loss: 0.0103\n",
      "Epoch 439/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0126 - val_loss: 0.0100\n",
      "Epoch 440/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0132 - val_loss: 0.0107\n",
      "Epoch 441/500\n",
      "8/8 - 0s - 8ms/step - loss: 0.0131 - val_loss: 0.0102\n",
      "Epoch 442/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0129 - val_loss: 0.0095\n",
      "Epoch 443/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0128 - val_loss: 0.0098\n",
      "Epoch 444/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0128 - val_loss: 0.0106\n",
      "Epoch 445/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0134 - val_loss: 0.0103\n",
      "Epoch 446/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0129 - val_loss: 0.0098\n",
      "Epoch 447/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0131 - val_loss: 0.0100\n",
      "Epoch 448/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0127 - val_loss: 0.0099\n",
      "Epoch 449/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0136 - val_loss: 0.0106\n",
      "Epoch 450/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0132 - val_loss: 0.0104\n",
      "Epoch 451/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0128 - val_loss: 0.0098\n",
      "Epoch 452/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0135 - val_loss: 0.0102\n",
      "Epoch 453/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0133 - val_loss: 0.0101\n",
      "Epoch 454/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0130 - val_loss: 0.0102\n",
      "Epoch 455/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0129 - val_loss: 0.0108\n",
      "Epoch 456/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0124 - val_loss: 0.0110\n",
      "Epoch 457/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0129 - val_loss: 0.0106\n",
      "Epoch 458/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0129 - val_loss: 0.0097\n",
      "Epoch 459/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0125 - val_loss: 0.0098\n",
      "Epoch 460/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0120 - val_loss: 0.0104\n",
      "Epoch 461/500\n",
      "8/8 - 0s - 10ms/step - loss: 0.0126 - val_loss: 0.0106\n",
      "Epoch 462/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0126 - val_loss: 0.0099\n",
      "Epoch 463/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0123 - val_loss: 0.0099\n",
      "Epoch 464/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0125 - val_loss: 0.0106\n",
      "Epoch 465/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0120 - val_loss: 0.0107\n",
      "Epoch 466/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0134 - val_loss: 0.0102\n",
      "Epoch 467/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0125 - val_loss: 0.0104\n",
      "Epoch 468/500\n",
      "8/8 - 0s - 8ms/step - loss: 0.0127 - val_loss: 0.0108\n",
      "Epoch 469/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0133 - val_loss: 0.0109\n",
      "Epoch 470/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0121 - val_loss: 0.0109\n",
      "Epoch 471/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0134 - val_loss: 0.0107\n",
      "Epoch 472/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0125 - val_loss: 0.0105\n",
      "Epoch 473/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0125 - val_loss: 0.0110\n",
      "Epoch 474/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0136 - val_loss: 0.0110\n",
      "Epoch 475/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0123 - val_loss: 0.0107\n",
      "Epoch 476/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0127 - val_loss: 0.0111\n",
      "Epoch 477/500\n",
      "8/8 - 0s - 11ms/step - loss: 0.0129 - val_loss: 0.0125\n",
      "Epoch 478/500\n",
      "8/8 - 0s - 18ms/step - loss: 0.0121 - val_loss: 0.0110\n",
      "Epoch 479/500\n",
      "8/8 - 0s - 13ms/step - loss: 0.0124 - val_loss: 0.0114\n",
      "Epoch 480/500\n",
      "8/8 - 0s - 12ms/step - loss: 0.0117 - val_loss: 0.0110\n",
      "Epoch 481/500\n",
      "8/8 - 0s - 12ms/step - loss: 0.0123 - val_loss: 0.0113\n",
      "Epoch 482/500\n",
      "8/8 - 0s - 10ms/step - loss: 0.0130 - val_loss: 0.0119\n",
      "Epoch 483/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0122 - val_loss: 0.0117\n",
      "Epoch 484/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0112 - val_loss: 0.0122\n",
      "Epoch 485/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0122 - val_loss: 0.0115\n",
      "Epoch 486/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0119 - val_loss: 0.0118\n",
      "Epoch 487/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0107 - val_loss: 0.0120\n",
      "Epoch 488/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0127 - val_loss: 0.0125\n",
      "Epoch 489/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0116 - val_loss: 0.0123\n",
      "Epoch 490/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0116 - val_loss: 0.0136\n",
      "Epoch 491/500\n",
      "8/8 - 0s - 8ms/step - loss: 0.0115 - val_loss: 0.0121\n",
      "Epoch 492/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0115 - val_loss: 0.0130\n",
      "Epoch 493/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0124 - val_loss: 0.0119\n",
      "Epoch 494/500\n",
      "8/8 - 0s - 11ms/step - loss: 0.0119 - val_loss: 0.0144\n",
      "Epoch 495/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0117 - val_loss: 0.0148\n",
      "Epoch 496/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0120 - val_loss: 0.0130\n",
      "Epoch 497/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0118 - val_loss: 0.0127\n",
      "Epoch 498/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0114 - val_loss: 0.0114\n",
      "Epoch 499/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0125 - val_loss: 0.0111\n",
      "Epoch 500/500\n",
      "8/8 - 0s - 9ms/step - loss: 0.0129 - val_loss: 0.0124\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.history.History at 0x360955e40>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create and fit the LSTM network, optimizer=adam, 50 neurons, 2 LSTM layers\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, return_sequences=True, input_shape=(look_back, 1)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(50))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "model.fit(trainX, trainY, epochs=500, batch_size=32, validation_data=(testX, testY), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n"
     ]
    }
   ],
   "source": [
    "# make predictions\n",
    "trainPredict = model.predict(trainX)\n",
    "testPredict = model.predict(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('../models/lstm_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invert predictions\n",
    "trainPredict = scaler.inverse_transform(trainPredict)\n",
    "trainY = scaler.inverse_transform([trainY])\n",
    "testPredict = scaler.inverse_transform(testPredict)\n",
    "testY = scaler.inverse_transform([testY])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Analyze and plot predictions\n",
    "\n",
    "Calculate RMSE for train and test, shift for plotting, and plot using Plotly. Note that the first 15 timesteps of both train and test are not accounted for in the prediction data since they were used to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score: 37.09 RMSE\n",
      "Test Score: 42.97 RMSE\n"
     ]
    }
   ],
   "source": [
    "# calculate root mean squared error\n",
    "trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\n",
    "print('Train Score: %.2f RMSE' % (trainScore))\n",
    "testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\n",
    "print('Test Score: %.2f RMSE' % (testScore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shift train predictions for plotting\n",
    "trainPredictPlot = np.empty_like(dataset)\n",
    "trainPredictPlot[:, :] = np.nan\n",
    "trainPredictPlot[look_back:len(trainPredict) + look_back, :] = trainPredict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shift test predictions for plotting\n",
    "testPredictPlot= np.empty_like(dataset)\n",
    "testPredictPlot[:, :] = np.nan\n",
    "testPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-2, :] = testPredict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "mode": "lines",
         "name": "Actual Data",
         "type": "scatter",
         "y": [
          779.53534,
          738.42413,
          741.9333,
          736.29333,
          731.44696,
          783.8993999999999,
          801.9030999999999,
          737.65625,
          682.86725,
          778.01483,
          720.2944300000001,
          764.0380000000001,
          807.8212999999998,
          803.03564,
          739.8966,
          778.28174,
          790.3799399999999,
          766.97675,
          775.7110599999999,
          773.8064,
          769.74316,
          769.5829499999999,
          786.32153,
          778.7888999999999,
          792.45435,
          763.81726,
          806.6888,
          888.0428,
          912.9139,
          929.5636,
          750.8795,
          753.1004,
          742.8684,
          765.5128000000001,
          745.14087,
          731.84906,
          737.8129,
          802.55536,
          761.0836,
          769.71655,
          827.0431,
          784.8896499999998,
          892.1064000000001,
          781.1029,
          761.3725999999999,
          839.83875,
          761.7999,
          761.11285,
          814.53217,
          729.8833600000002,
          774.7037000000001,
          755.82556,
          760.9177,
          758.64935,
          765.5382,
          750.0122000000001,
          799.3717,
          759.1179000000001,
          807.7095,
          781.23956,
          793.1362000000001,
          773.3901400000001,
          807.1425999999999,
          809.65875,
          775.64667,
          810.0696,
          777.7804,
          812.9107999999999,
          863.68146,
          782.8091999999999,
          794.82446,
          793.0781,
          829.9197999999999,
          757.5804,
          762.94727,
          815.7253399999998,
          828.2929,
          746.1347,
          979.03687,
          807.9018599999999,
          837.9640999999999,
          843.12646,
          864.8735,
          747.812,
          773.926,
          811.81415,
          787.9254,
          740.1255,
          828.8951999999999,
          819.7054,
          813.4805,
          846.9989,
          789.5592,
          814.1469,
          829.0668,
          821.93823,
          815.7347999999998,
          827.3362,
          826.8686,
          846.60944,
          810.25433,
          944.5558,
          859.783,
          776.1410999999999,
          860.7285,
          784.1573,
          776.9295999999999,
          829.2263,
          818.2905,
          863.39655,
          848.26624,
          838.15735,
          820.5433,
          843.01324,
          903.2396,
          920.8287000000001,
          701.6564,
          734.9771,
          857.1394000000001,
          957.8990499999999,
          890.17126,
          826.97186,
          823.71234,
          823.6747,
          830.22516,
          827.4907000000001,
          826.2654,
          838.7302,
          830.75714,
          853.87317,
          876.07434,
          843.78564,
          826.93384,
          782.0467,
          839.0111,
          840.1141999999999,
          809.34247,
          846.3443,
          845.2434,
          840.3825999999999,
          841.9008,
          844.24384,
          839.3294999999999,
          837.2445699999998,
          844.5326,
          837.3416,
          845.1107,
          862.8172,
          861.8744,
          849.7458,
          838.27045,
          830.01953,
          835.5708,
          833.964,
          847.4599000000001,
          848.9426,
          846.0026999999999,
          852.79425,
          840.45825,
          858.9449,
          843.8492,
          829.75806,
          834.30444,
          845.4909999999999,
          842.97644,
          849.8517,
          842.9386000000001,
          743.20465,
          820.0958,
          948.1581399999999,
          679.17847,
          728.18353,
          745.0368999999998,
          625.2242,
          786.2057500000001,
          793.80206,
          796.18207,
          788.5788,
          795.98096,
          805.4753000000001,
          809.70374,
          828.09735,
          808.27136,
          807.0099,
          812.7288999999998,
          811.39453,
          811.8516,
          807.05536,
          829.0688500000001,
          836.55975,
          828.9171,
          835.9408,
          848.8149,
          830.27673,
          830.8216,
          829.38385,
          814.40857,
          838.7331999999999,
          842.80786,
          828.7443,
          833.2852,
          836.56195,
          837.8084,
          832.99664,
          840.33295,
          845.8898999999999,
          854.14716,
          847.1035999999999,
          869.7076,
          849.6133,
          850.3320000000001,
          851.0433,
          858.1084599999999,
          893.64465,
          856.161,
          871.5333,
          883.9967,
          806.8979,
          850.83575,
          944.0604,
          966.42816,
          977.6059,
          902.7519,
          776.9396399999999,
          760.2787999999999,
          737.2519,
          780.7937600000001,
          783.98224,
          778.1462,
          788.54425,
          786.12335,
          788.6476,
          788.3852,
          788.5048,
          813.4975600000001,
          790.72845,
          802.1187000000001,
          808.6795,
          808.1745999999999,
          803.2641,
          820.6401,
          820.55786,
          814.7866,
          821.8431,
          854.5204,
          823.9852,
          818.9256,
          836.8924600000001,
          822.72876,
          829.7194,
          839.76807,
          816.4167,
          826.2480499999999,
          845.6341000000001,
          830.9395999999999,
          814.9132000000001,
          837.6171,
          827.0943,
          831.05414,
          841.5177,
          817.1129000000001,
          849.0217,
          840.69794,
          838.90344,
          851.06433,
          842.9311,
          848.3822,
          851.82587,
          813.0235,
          846.92316,
          887.2899,
          855.2439,
          727.6456999999999,
          885.7745400000001,
          945.8258700000001,
          960.0337,
          930.5928000000001,
          905.5867300000001,
          940.5631999999999,
          867.36017,
          820.31946,
          838.007,
          828.22003,
          824.91364,
          839.00946,
          845.1768,
          834.329,
          858.4682,
          822.0428,
          839.1544,
          824.6978,
          831.4095,
          844.6448000000001,
          850.7153,
          855.60126,
          854.1229,
          827.1391,
          844.0580399999999,
          845.94104,
          841.1847,
          872.64355,
          825.82367,
          857.70386,
          874.7331500000001,
          876.8169,
          846.2484,
          860.6264999999999,
          858.6454,
          858.0603,
          865.4502600000001,
          864.37646,
          881.6574,
          915.58887,
          946.7455,
          804.004,
          833.9113,
          877.5230000000001,
          894.8965,
          888.1239600000001,
          837.4727,
          871.6311,
          864.1263,
          865.25696,
          893.01465,
          850.8103,
          823.4797,
          869.7585399999999,
          876.8725599999999,
          860.8418999999999,
          862.7819,
          888.0641,
          862.12354,
          838.5176999999999,
          859.83154,
          843.36633,
          868.70624,
          883.35785,
          875.4683,
          814.98004,
          899.4535000000001,
          957.9405999999999,
          805.1623500000001,
          726.8116,
          709.9305,
          731.0368700000001,
          752.65985,
          749.9105,
          804.1374,
          808.4432399999998,
          784.3592,
          813.3298,
          822.6051,
          824.1515,
          815.5497,
          821.66064,
          814.3937,
          815.4064,
          818.0148,
          829.8501600000001,
          829.9358000000001,
          844.6734,
          833.6204,
          838.2255,
          857.9733999999999
         ]
        },
        {
         "mode": "lines",
         "name": "Train Predictions",
         "type": "scatter",
         "y": [
          null,
          null,
          null,
          null,
          null,
          770.0472412109375,
          769.7385864257812,
          775.9412231445312,
          779.0338745117188,
          775.0553588867188,
          771.9944458007812,
          766.429931640625,
          765.9051513671875,
          769.335693359375,
          784.0616455078125,
          783.894775390625,
          787.9512939453125,
          787.9570922851562,
          784.7001342773438,
          783.8057250976562,
          785.758544921875,
          786.0211181640625,
          783.67041015625,
          785.3668212890625,
          785.8176879882812,
          788.2685546875,
          787.0266723632812,
          791.8558349609375,
          800.6161499023438,
          817.9971313476562,
          834.1997680664062,
          759.5883178710938,
          761.5673828125,
          796.5972900390625,
          783.6394653320312,
          773.4353637695312,
          772.9093627929688,
          769.8323974609375,
          773.8861083984375,
          775.5902099609375,
          780.0394897460938,
          784.7833251953125,
          792.3291625976562,
          802.8347778320312,
          797.6879272460938,
          811.25341796875,
          789.3479614257812,
          789.9918212890625,
          791.09814453125,
          784.9542846679688,
          784.6364135742188,
          783.9694213867188,
          774.7809448242188,
          777.8920288085938,
          775.3446044921875,
          779.3563842773438,
          777.1618041992188,
          780.824951171875,
          780.6302490234375,
          786.6895751953125,
          785.3733520507812,
          793.0497436523438,
          788.0654907226562,
          796.1320190429688,
          794.1539916992188,
          795.1363525390625,
          796.9251708984375,
          795.3602294921875,
          799.0421142578125,
          801.6414794921875,
          801.7101440429688,
          806.79150390625,
          794.927490234375,
          809.6724853515625,
          793.3156127929688,
          795.6322631835938,
          789.6248168945312,
          796.9615478515625,
          790.5070190429688,
          812.908203125,
          798.3370971679688,
          826.5515747070312,
          783.2391967773438,
          774.9953002929688,
          799.5439453125,
          825.90576171875,
          798.6902465820312,
          790.2506103515625,
          786.2138671875,
          791.589111328125,
          796.7182006835938,
          797.5232543945312,
          803.1619262695312,
          807.8560791015625,
          815.8175048828125,
          807.2706909179688,
          813.1893310546875,
          807.8291625976562,
          814.2615356445312,
          816.5064697265625,
          819.7291870117188,
          812.7534790039062,
          844.9046630859375,
          812.1834716796875,
          845.1091918945312,
          840.5615234375,
          781.5269165039062,
          808.16748046875,
          794.0879516601562,
          807.3684692382812,
          808.1805419921875,
          812.3076782226562,
          828.9602661132812,
          821.33251953125,
          851.2538452148438,
          847.423095703125,
          846.7993774414062,
          763.8186645507812,
          826.0440063476562,
          915.6640014648438,
          870.6289672851562,
          814.6863403320312,
          831.8892822265625,
          807.8485717773438,
          799.5707397460938,
          853.4962768554688,
          817.96142578125,
          818.2144775390625,
          820.7899780273438,
          820.4922485351562,
          827.3203125,
          831.8890991210938,
          830.0757446289062,
          838.464111328125,
          819.3001708984375,
          851.7386474609375,
          817.5401611328125,
          810.4597778320312,
          813.8931884765625,
          823.67724609375,
          826.8101196289062,
          822.819091796875,
          836.78955078125,
          834.107421875,
          832.712646484375,
          834.4466552734375,
          831.2262573242188,
          833.8574829101562,
          836.1177978515625,
          839.0103149414062,
          839.2012329101562,
          844.0509643554688,
          851.5020141601562,
          846.37841796875,
          831.3069458007812,
          829.9985961914062,
          827.1400756835938,
          831.8812866210938,
          835.8167724609375,
          836.7967529296875,
          848.8992919921875,
          833.7634887695312,
          842.4682006835938,
          834.7451171875,
          841.7501831054688,
          830.1753540039062,
          830.343994140625,
          830.4683227539062,
          805.2217407226562,
          830.3482055664062,
          839.6223754882812,
          753.59228515625,
          803.0402221679688,
          662.0588989257812,
          777.6527099609375,
          753.40625,
          757.0574951171875,
          768.5192260742188,
          777.5714721679688,
          795.2330322265625,
          797.0657958984375,
          799.215087890625,
          802.8209838867188,
          804.383056640625,
          808.072998046875,
          807.3726196289062,
          809.5847778320312,
          806.4873046875,
          805.8851928710938,
          809.9210205078125,
          811.4559326171875,
          813.8732299804688,
          817.5752563476562,
          825.8900756835938,
          824.1980590820312,
          827.23486328125,
          825.298828125,
          824.8001098632812,
          823.3861694335938,
          820.9503173828125,
          820.6173706054688,
          821.4349365234375,
          827.3707885742188,
          828.4932861328125,
          823.6650390625,
          827.990966796875,
          829.19287109375,
          832.937255859375,
          831.0878295898438,
          846.1171264648438,
          835.5531005859375,
          857.7623901367188,
          844.1934814453125,
          866.0786743164062,
          860.2311401367188,
          834.7721557617188,
          881.5335083007812,
          858.9470825195312,
          839.3887329101562,
          879.9862670898438,
          904.5287475585938,
          886.7999877929688,
          884.3400268554688,
          763.1912231445312,
          754.5491943359375,
          758.2059326171875,
          793.7362060546875,
          786.3822021484375,
          780.4943237304688,
          782.5111694335938,
          784.9545288085938,
          789.9359130859375,
          791.3901977539062,
          791.4010009765625,
          792.9097900390625,
          795.4298095703125,
          795.58935546875,
          798.5672607421875,
          798.5162963867188,
          803.00390625,
          801.0719604492188,
          805.32666015625,
          806.951171875,
          808.3571166992188,
          809.8199462890625,
          818.6683959960938,
          814.6322631835938,
          819.5956420898438,
          820.3273315429688,
          824.0223388671875,
          819.6503295898438,
          819.0924072265625,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null
         ]
        },
        {
         "mode": "lines",
         "name": "Test Predictions",
         "type": "scatter",
         "y": [
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          821.427490234375,
          819.3514404296875,
          820.3153686523438,
          827.5308227539062,
          821.6281127929688,
          829.8046875,
          826.2962646484375,
          835.121337890625,
          837.6105346679688,
          836.0156860351562,
          828.9378662109375,
          842.8670654296875,
          843.3654174804688,
          830.8056640625,
          805.5977172851562,
          861.6788940429688,
          879.0420532226562,
          847.1371459960938,
          849.5632934570312,
          827.593994140625,
          781.8273315429688,
          786.8719482421875,
          753.7056274414062,
          843.04345703125,
          842.2953491210938,
          836.6053466796875,
          820.6905517578125,
          826.107666015625,
          823.1055297851562,
          831.216064453125,
          821.5319213867188,
          840.455810546875,
          819.627685546875,
          834.7684326171875,
          821.7257690429688,
          828.7083129882812,
          828.9358520507812,
          834.2951049804688,
          834.1646728515625,
          850.1728515625,
          838.2691040039062,
          836.9722290039062,
          836.5390625,
          822.3511352539062,
          854.0745239257812,
          838.411376953125,
          866.5194091796875,
          834.9994506835938,
          881.7650756835938,
          862.9553833007812,
          877.9537353515625,
          852.2096557617188,
          861.4302978515625,
          874.1707763671875,
          887.344482421875,
          912.1812744140625,
          749.79443359375,
          779.8548583984375,
          805.9949340820312,
          888.9595947265625,
          836.699462890625,
          837.1570434570312,
          917.6722412109375,
          840.4605712890625,
          896.3204345703125,
          859.838623046875,
          841.0684814453125,
          876.385986328125,
          888.8165283203125,
          874.6610717773438,
          845.4093627929688,
          845.12939453125,
          894.0407104492188,
          860.7521362304688,
          872.7661743164062,
          880.5861206054688,
          861.9932250976562,
          864.1134033203125,
          844.8253784179688,
          862.5953369140625,
          831.6407470703125,
          917.3584594726562,
          893.0384521484375,
          749.7044067382812,
          855.3048706054688,
          800.3959350585938,
          781.6443481445312,
          765.6598510742188,
          765.0541381835938,
          772.0006713867188,
          781.659423828125,
          789.2530517578125,
          794.570068359375,
          802.6870727539062,
          806.1771240234375,
          805.9593505859375,
          812.9853515625,
          812.5499267578125,
          813.21142578125,
          810.8536376953125,
          813.8325805664062,
          813.3321533203125,
          818.6233520507812,
          818.7872924804688,
          null,
          null
         ]
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "LSTM Predictions vs Actual Data for Middle Point"
        },
        "xaxis": {
         "ticktext": [
          2015,
          2016,
          2017,
          2018,
          2019,
          2020,
          2021,
          2022,
          2023,
          2024
         ],
         "tickvals": [
          0,
          13,
          48,
          95,
          143,
          191,
          246,
          299,
          326,
          354
         ],
         "title": {
          "text": "Time"
         }
        },
        "yaxis": {
         "title": {
          "text": "Velocity"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create traces\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(y=scaler.inverse_transform(dataset).flatten(), mode='lines', name='Actual Data'))\n",
    "fig.add_trace(go.Scatter(y=trainPredictPlot.flatten(), mode='lines', name='Train Predictions'))\n",
    "fig.add_trace(go.Scatter(y=testPredictPlot.flatten(), mode='lines', name='Test Predictions'))\n",
    "\n",
    "# Update layout for dark theme\n",
    "fig.update_layout(title='LSTM Predictions vs Actual Data for ' + point_name.capitalize() + ' Point', xaxis_title='Time', yaxis_title='Velocity')\n",
    "# Update x-axis to show only one tick per year\n",
    "years = pd.to_datetime(df['time']).dt.year\n",
    "unique_years = sorted(years.unique())\n",
    "fig.update_xaxes(tickvals=[df.index[years == year][0] for year in unique_years], ticktext=unique_years)\n",
    "fig.write_image('../figures/8_LSTM_preds_' + point_name + '.png')\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlgeo-jensencc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
